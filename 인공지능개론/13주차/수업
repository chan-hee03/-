다음 시간 온라인
기말범위 1주차~13주차

복습
MLP : 
Cnn :

pretrained model : 이미 학습된 모델
  앞부분은 일반적인 특징을 추출하기 때문에 이미 검증된 부분을 사용하는것

fine-tuning : 미세조정, 

argumentation : 데이터가 부족, 불균형을 해결하기 위해. 이미지를 조금 밀어내거나 사이즈를 조절하는 방식으로 데이터를 늘림.
  속성을 유지하면서 다양성을 높여야 함




Rnn : 과거의 상태와 현재의 입력를 고려하는 방법
  time sequence data, time series data : 순환 데이터, 시계열 데이터
  예측, 분류, 회귀, 단기/장기 예측을 나눠서 판단

  순환데이터 필요 -> sliding window

  privious status : 이전 상태

  일대일/일대다... : 입력과 출력의 갯수에 따라 다름

  타입스텝

  네트워크층이 길어지면 gradiant vanishing 발생
  아니면 폭증하는 경우도 있음 -> 일정값을 넘을경우 잘라내는 방식으로 해결

LSTM : Rnn의 gradiant vanishing 해결 목적, 장/단기 메모리
  c(셀) 추가 : 장기 기억을 유지하는 데이터
  어떤 데이터를 롱텀에 추가시킬 것인가
  게이트 : 셀에 보낼 기억/삭제할 기억을 정리함
  입력게이트 : 현재입력과 히든상태를 받아 셀에 보낼것인지 조절함 
  삭제게이트 : 현재상태와 히든상태를 .... 히든을 삭제함
  입력과 삭제게이트를 비교하여 셀에 추가/삭제를 결정
  출력 게이트 : 다음의 히든 상태를 결정함

  여전히 gradiant vanishing가 일부 일어남, 느림

GRU : 속도를 개선함



